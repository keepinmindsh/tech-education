## 1초에만 몇 억 번씩 0과 1을 헤아리는 CPU

> 컴퓨터 CPU의 가장 기본적인 역할은 연산/계산 작업이다. 이를테면 사용자가 '0 + 1'이라는 명령을 내리면 CPU는 이를 받아들여 계산을 한 후 '1'이라는 결과를 영상 출력 장치(모니터 등)로 보여준다.

- **< CPU는 수많은 0과 1로 이루어진 데이터를 연산해 다양한 결과물을 도출한다>**

왜냐하면 컴퓨터의 내부에서 이동하는 데이터는 어차피 '0'과 '1'로만 구성된 디지털 신호의 조합이기 때문이다. 
예를 들어, 사용자가 숫자 '3'을 컴퓨터에 입력하면 CPU는 이를 '00011'이라는 0과 1의 조합으로 인식한다. 
만약 단순한 숫자 데이터가 아닌 영상이나 음악 등의 복잡한 데이터를 CPU가 인식, 처리할 수 있는 디지털 신호로 구성하려면 
수많은 0과 1의 조합이 필요하다.    

이를 테면 5MB 용량의 음악 파일 1개가 있다면 CPU는 이를 총 4천만 개 정도의 0과 1이 조합된 집합체로 인식한다. 
사용자가 PC 상에서 음악 파일 하나를 클릭하여 재생하는 순간, 
CPU 내부에서는 '1100011000111101100011000110011001110~'등으로 한없이 이어지는 연산 작업이 초당 몇 천만 번, 
혹은 몇 억 번씩 이루어 지고 있는 셈이다.

## 진수 

- 2진수 

```text

01001100(2)



```

- 10 진수

> 1,2,3,4,5,6,7,8,9,10

```text

76

```

- 16 진수 

> 1,2,3,4,5,6,7,8,9,A,B,C,D,E,F  
> 헥사값은 16진법을 표기하기 위한 값, 영어로는 Hexadecimal

```text

0X4C

표현 방법 

(16), hex, 0X

```

## 진법 변환 

> 2진수는 0과1 로만 이루어진 수 두가지 숫자만 표현가능합니다. 영어로는 Binary.   
8진수는 0~7로 이루어진 수로 8가지 숫자가 표현가능하며 영어로는 Octal.
10진수는 우리가 주로 사용하는 0~9로 이루어진 수로 10가지 숫자가 표현가능하며 영어로는 Decimal.  
16진수는 0~15로 이루어진수로 16가지 숫자를 표현 가능하며 영어로는 Hexadecimal.

- 16진법 -> 2진법 

```text

0X10 -> 0001 0000 

```

- 10진법 -> 16진법 

```text

16 -> 0001(1) 0000(0) -> 0X10

```

- 16진법 -> 10 진법 

```text

0x10 -> 0001(16) 0000 -> 16

```

## 왜 진법이 나왔을까? 

- 사람이 좀 더 쉽게 이해할 수 있도록 하기 위해서 

> 만약 2 진수로 20억을 표현하라고 한다면...   
>   실제로 1940년대에는 모든 수를 이진수로 사용했었음. 

- 2진법보다 자릿수를 덜 차지하기 때문에 효율이 좋아진다는 점

- 16진수에서 2진수로의 변환이 쉬움

## bit 

> 2진법 1자리를 표현하는 정보량의 최소단위이다.  
> 컴퓨터는 0과 1의 조합으로 수의 계산과 논리 계산을 한다. 이는 결국 1비트로 0과 1 즉 O , X를 표현하는 것과 같다.

- 1 bit는 0과 1로 이루어짐. 

- 8 bit의 최대 값은 255이다. 

## Byte 

>  1바이트란? 2의 8제곱 즉  2^8 으로 256종류의 정보를 나타낼 수 있는 정보의 양    
>  8비트 즉 1바이트가 생겨날 당시에는 256으로 충분했다고 하니 그것이 기준이 되어 지금까지 오고 있다. 

## Java Primitive Types 

```text

int - A 32-bit ( 4-byte ) integer value
short - A 16-bit ( 2-byte ) integer value
long - A 64-bit ( 8-byte ) integer value
byte - An 8-bit ( 1-byte ) integer value
float - A 32-bit ( 4-byte ) integer value
double - A 64-bit ( 8-byte ) integer value
char - A 16-bit character using the Unicode encoding scheme

```

## 32 bit, 64bit 컴퓨터에서의 데이터 타입

> 우리는 x86_64 프로세서를 사용합니다. 하지만 64bit 체계의 운영체제이나 int size는 4bit로 나옵니다.
이러한 이유를 살펴보면 int형의 크기는 컴파일러에서 정하는거 같지만 실질적으로 크기를 정하는 것은 System Vendor의 몫이라고 합니다.
그러므로 Intel의 경우에는 Intel CPU에선 int형이 무조건 4byte라고 합니다.
보통 사람들은 int형이 16bit 에서는 2byte, 32bite 에서는 4byte를 가지고 있으니,
당연히 64bit 에서는 8byte의 크기를 가지고 있다고 아는 사람들이 많지만 64bit에서는 int형의 크기가 반드시 8byte여야 한다는 규약은 없다고 합니다.
또한 만일 64bit의 int형을 8byte로 설정 할 경우 32bit에서 64bit로 가면서 모든 프로그램을 수정해야 할 수 있기 때문이라고 합니다.
그리고 포인터 변수와 같은 경우에는 자료형에 상관 없이 8byte로 할당 되어 있습니다.
※ 32bit 와 마찬가지로 64bit 컴퓨터에서는 메모리 주소의 범위가 0 ~ 2^64 - 1에 해당하기 때문 입니다.

## 유니코드 

> 기본적으로 컴퓨터는 숫자만 처리합니다. 글자나 다른 문자에도 숫자를 지정하여 저장합니다. 
> 유니코드가 개발되기 전에는 이러한 숫자를 지정하기 위해 수백 가지의 다른 기호화 시스템을 사용했습니다. 
> 단일 기호화 방법으로는 모든 문자를 포함할 수 없었습니다. 
> 예를 들어 유럽 연합에서만 보더라도 모든 각 나라별 언어를 처리하려면 여러 개의 다른 기호화 방법이 필요합니다. 
> 영어와 같은 단일 언어의 경우도 공통적으로 사용되는 모든 글자, 문장 부호 및 테크니컬 기호에 맞는 단일 기호화 
> 방법을 갖고 있지 못하였습니다. 
> 이러한 기호화 시스템은 또한 다른 기호화 시스템과 충돌합니다. 
> 즉 두 가지 기호화 방법이 두 개의 다른 문자에 대해 같은 번호를 사용하거나 같은 문자에 대해 다른 번호를 사용할 수 있습니다. 
> 주어진 모든 컴퓨터(특히 서버)는 서로 다른 여러 가지 기호화 방법을 지원해야 합니다. 
> 그러나, **데이터를 서로 다른 기호화 방법이나 플랫폼 간에 전달할 때마다 그 데이터는 항상 손상의 위험을 겪게 됩니다.**

- 유니코드는 글자와 코드가 1:1 매핑되어 있는 ‘코드표'이다.

```text

&#xC0B0;&#xD558;

&#x25;

&#xAC00;

&#x41;

```

- UTF-8은 유니코드를 인코딩(encoding)하는 방식이다. 전세계에서 사용하는 약속이다.

> 유니코드를 통해 코드표가 정의되었다. 남은 것은 그 ‘코드'가 컴퓨터에 어떻게 저장되어야 하는 것이다. 
> 다른 말로 인코딩(encoding)이라고 하는데, 컴퓨터가 이해할 수 있는 형태로 바꿔주는 것이다

- 유니코드는 국제표준 문자표이고 UTF-8은 인코딩 방식이다. 